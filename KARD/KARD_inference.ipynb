{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import TextStreamer, GenerationConfig\n",
        "\n",
        "class InferenceModelLoader:\n",
        "    def __init__(self, base_model_name, lora_adapter_path, load_in_4bit=True):\n",
        "        self.base_model_name = base_model_name\n",
        "        self.lora_adapter_path = lora_adapter_path\n",
        "        self.load_in_4bit = load_in_4bit\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        print(\"Загрузка базовой модели...\")\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.base_model_name,\n",
        "            max_seq_length=2047,\n",
        "            dtype=None,\n",
        "            load_in_4bit=self.load_in_4bit,\n",
        "        )\n",
        "\n",
        "        print(f\"Применение LoRA-адаптера из '{self.lora_adapter_path}'...\")\n",
        "        self.model = PeftModel.from_pretrained(model, self.lora_adapter_path)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        print(\"Подготовка модели для инференса...\")\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "        self.model.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"left\"\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, dataset_path, qdrant_url = \"https://a8e5d73e-0e72-4f33-bb7e-b5a8ea2a44ed.europe-west3-0.gcp.cloud.qdrant.io\", qdrant_api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.amfoIHUvKV1c5vpcpDSajp3kJKMTws9qaF1m9iHio9I\",\n",
        "                 collection_name=\"ru_wiki_passages_test\", embed_model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.qdrant = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
        "        self.collection_name = collection_name\n",
        "        self.embed_model = SentenceTransformer(embed_model_name)\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        print(f\"Загрузка и подготовка данных из '{self.dataset_path}'...\")\n",
        "        dataset = load_dataset(\"json\", data_files=self.dataset_path, split=\"train\")\n",
        "        dataset = dataset.filter(lambda example: 'ground_truth_answer' in example and example['ground_truth_answer'] is not None)\n",
        "        dataset = dataset.map(self.build_prompt)\n",
        "        return dataset\n",
        "\n",
        "    def retrieve_context(self, query, top_k=5):\n",
        "        query_vector = self.embed_model.encode(query).tolist()\n",
        "        search_result = self.qdrant.search(\n",
        "            collection_name=self.collection_name,\n",
        "            query_vector=query_vector,\n",
        "            limit=top_k,\n",
        "        )\n",
        "        passages = [hit.payload.get(\"text\", \"\") for hit in search_result]\n",
        "        return \"\\n\".join(passages)\n",
        "\n",
        "    def build_prompt(self, sample):\n",
        "        question = sample.get(\"original_question\")\n",
        "        retrieved_context = self.retrieve_context(question)\n",
        "\n",
        "        prompt_template = (\n",
        "            f\"Инструкция для эксперта-аналитика**\\n\"\n",
        "            f\"Тема: '{sample.get('subject', sample.get('domain', ''))}'.\\n\"\n",
        "            f\"Контекст из базы знаний:\\n{retrieved_context}\\n\\n\"\n",
        "            f\"Ваша задача — выполнить строгий логический анализ предоставленной задачи. Следуйте этому алгоритму:\\n\"\n",
        "            f\"1.  **Анализ Задачи: Кратко определите основной вопрос и какой логический или математический принцип нужно применить.\\n\"\n",
        "            f\"2.  Пошаговая Оценка Вариантов: Систематически рассмотрите КАЖДЫЙ вариант ответа (A, B, C, D). Для каждого варианта предоставьте четкое и лаконичное объяснение, почему он является верным или неверным в контексте задачи.\\n\"\n",
        "            f\"3.  Синтез и Вывод: На основе пошагового анализа, сделайте окончательный вывод и выберите единственно правильный ответ.\\n\\n\"\n",
        "            f\"Формат вывода**\\n\"\n",
        "            f\"Ваш ответ ДОЛЖЕН СТРОГО соответствовать формату ниже, без лишних вступлений или заключений:\\n\"\n",
        "            f\"**Рассуждение:**\\n\"\n",
        "            f\"[Здесь ваш детальный анализ по шагам 1-3]\\n\"\n",
        "            f\"**Ответ: [Здесь ОДНА буква: A, B, C или D]\\n\\n\"\n",
        "            f\"---\"\n",
        "            f\"**Задача для анализа:**\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            f\"**Рассуждение:**\\n\"\n",
        "        )\n",
        "        return {\n",
        "            \"prompt\": prompt_template,\n",
        "            \"original_question\": question\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Xies6k8-_akr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelEvaluator:\n",
        "    def __init__(self, model_loader, data_processor, batch_size=8, output_path=\"evaluation_results.jsonl\"):\n",
        "        self.model_loader = model_loader\n",
        "        self.data_processor = data_processor\n",
        "        self.stats = {\"correct\": 0, \"incorrect\": 0, \"no_answer\": 0}\n",
        "        self.batch_size = batch_size\n",
        "        self.output_path = output_path\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_answer(generated_text):\n",
        "        match = re.search(r\"\\*\\*Ответ:\\s*([A-D])\", generated_text, re.IGNORECASE)\n",
        "        return match.group(1).upper() if match else None\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_reasoning(generated_text):\n",
        "        match = re.search(r\"\\*\\*Ответ:\", generated_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return generated_text[:match.start()].strip()\n",
        "        return generated_text.strip()\n",
        "\n",
        "    def run(self, verbose=False):\n",
        "        model = self.model_loader.model\n",
        "        tokenizer = self.model_loader.tokenizer\n",
        "        dataset = self.data_processor.load_and_prepare_data()\n",
        "\n",
        "        with open(self.output_path, 'w', encoding='utf-8') as log_file:\n",
        "            print(f\"Результаты будут сохранены в: {self.output_path}\")\n",
        "            progress_bar = tqdm(dataset.iter(batch_size=self.batch_size), desc=\"Оценка модели\", total=len(dataset) // self.batch_size)\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                messages_batch = batch[\"messages\"]\n",
        "\n",
        "                chat_templates = [\n",
        "                    tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
        "                    for msg in messages_batch\n",
        "                ]\n",
        "\n",
        "                encoded = tokenizer(\n",
        "                    chat_templates,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=2047,\n",
        "                    return_attention_mask=True\n",
        "                )\n",
        "\n",
        "                input_ids = encoded[\"input_ids\"].to(\"cuda\")\n",
        "                attention_mask = encoded[\"attention_mask\"].to(\"cuda\")\n",
        "\n",
        "                # with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=2047,\n",
        "                    use_cache=True,\n",
        "                    do_sample=False,\n",
        "                    eos_token_id=tokenizer.pad_token_id,\n",
        "                    repetition_penalty=1.0\n",
        "                )\n",
        "\n",
        "                generated_texts = tokenizer.batch_decode(outputs[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "                for i in range(len(generated_texts)):\n",
        "                    ground_truth = batch['model_answer'][i]\n",
        "                    full_generated_text = generated_texts[i]\n",
        "                    model_answer = self.extract_answer(full_generated_text)\n",
        "                    model_reasoning = self.extract_reasoning(full_generated_text)\n",
        "\n",
        "                    if model_answer is None:\n",
        "                        self.stats[\"no_answer\"] += 1\n",
        "                        result = \"NO_ANSWER\"\n",
        "                    elif model_answer == ground_truth:\n",
        "                        self.stats[\"correct\"] += 1\n",
        "                        result = \"CORRECT\"\n",
        "                    else:\n",
        "                        self.stats[\"incorrect\"] += 1\n",
        "                        result = \"INCORRECT\"\n",
        "\n",
        "                    log_entry = {\n",
        "                        \"id\": batch.get('id', ['N/A']*len(generated_texts))[i],\n",
        "                        \"original_question\": batch['original_question'][i],\n",
        "                        \"ground_truth\": ground_truth,\n",
        "                        \"model_reasoning\": model_reasoning,\n",
        "                        \"model_answer\": model_answer,\n",
        "                        \"result\": result,\n",
        "                        \"full_generated_text\": full_generated_text\n",
        "                    }\n",
        "                    log_file.write(json.dumps(log_entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "                progress_bar.set_postfix({\n",
        "                    '✅': self.stats['correct'],\n",
        "                    '❌': self.stats['incorrect'],\n",
        "                    '❓': self.stats['no_answer']\n",
        "                })\n",
        "\n",
        "        self.print_summary()\n",
        "\n",
        "\n",
        "    def print_summary(self):\n",
        "        total = sum(self.stats.values())\n",
        "        if total == 0:\n",
        "            print(\"Не было обработано ни одного сэмпла.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"#\" * 20 + \" Итоги оценки \" + \"#\" * 20)\n",
        "        print(f\"Всего обработано: {total}\")\n",
        "        print(f\"✅ Верных: {self.stats['correct']} ({self.stats['correct']/total:.2%})\")\n",
        "        print(f\"❌ Неверных: {self.stats['incorrect']} ({self.stats['incorrect']/total:.2%})\")\n",
        "        print(f\"❓ Без ответа: {self.stats['no_answer']} ({self.stats['no_answer']/total:.2%})\")\n",
        "        print(f\"Результаты в: {self.output_path}\")\n",
        "        print(\"#\" * 60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    BASE_MODEL_NAME = \"\"\n",
        "    ADAPTER_PATH = \"\"\n",
        "    DATASET_PATH = \"\"\n",
        "    OUTPUT_LOG_PATH = \"\"\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    try:\n",
        "        model_loader = InferenceModelLoader(BASE_MODEL_NAME, ADAPTER_PATH)\n",
        "        data_processor = DatasetProcessor(DATASET_PATH)\n",
        "        evaluator = ModelEvaluator(model_loader, data_processor, batch_size=BATCH_SIZE, output_path=OUTPUT_LOG_PATH)\n",
        "        evaluator.run(verbose=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Ошибка: {e}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Проверьте пути, LoRA и BATCH_SIZE. При ошибке CUDA OOM — уменьшите батч.\")\n"
      ],
      "metadata": {
        "id": "0pFHwHAtAhYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}