{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab6aa0-97cd-428e-8f49-7f309759b6c8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T15:24:58.365479Z",
     "iopub.status.idle": "2025-07-24T15:24:58.365758Z",
     "shell.execute_reply": "2025-07-24T15:24:58.365635Z",
     "shell.execute_reply.started": "2025-07-24T15:24:58.365620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from unsloth import FastLanguageModel\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "class InferenceModelLoader:\n",
    "    def __init__(self, base_model_name, lora_adapter_path, load_in_4bit=True):\n",
    "        self.base_model_name = base_model_name\n",
    "        self.lora_adapter_path = lora_adapter_path\n",
    "        self.load_in_4bit = load_in_4bit\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        print(\"Загрузка базовой модели...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.base_model_name,\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "        )\n",
    "\n",
    "        print(f\"Применение LoRA-адаптера из '{self.lora_adapter_path}'...\")\n",
    "        self.model = PeftModel.from_pretrained(model, self.lora_adapter_path)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        print(\"Подготовка модели для инференса...\")\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "class DatasetProcessor:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        print(f\"Загрузка и подготовка данных из '{self.dataset_path}'...\")\n",
    "        dataset = load_dataset(\"json\", data_files=self.dataset_path, split=\"train\")\n",
    "        dataset = dataset.filter(lambda x: 'ground_truth_answer' in x and x['ground_truth_answer'] is not None)\n",
    "        dataset = dataset.map(lambda x: {\"model_answer\": x[\"model_extracted_answer\"]})\n",
    "        dataset = dataset.map(self.build_prompt)\n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def build_prompt(sample):\n",
    "        original_question = sample.get(\"original_question\") or sample.get(\"text\")\n",
    "        cleaned_reasoning = sample.get(\"cleaned_reasoning\")\n",
    "        model_answer = sample.get(\"model_extracted_answer\")\n",
    "\n",
    "        if not original_question or not cleaned_reasoning or not model_answer:\n",
    "            return sample  # Пропускаем, если чего-то нет\n",
    "\n",
    "        system_prompt = (\n",
    "            \"Реши задачу пошагово на русском языке, объясняя каждое действие. \"\n",
    "            \"Покажи все размышления, вычисления, логические выводы и проверь расчеты перед окончательным ответом.\\n\"\n",
    "            \"Твой ответ должен быть кратким, точным и содержать только следующие блоки:\\n\\n\"\n",
    "            \"Рассуждение:\\n\"\n",
    "            \"[Шаг 1 – объяснение и вычисления]\\n\"\n",
    "            \"[Шаг 2 – проверка расчетов]\\n\"\n",
    "            \"Ответ: [Только одна буква: A, B, C или D].\"\n",
    "        )\n",
    "\n",
    "        user_prompt = f\"Задача, которую нужно решить: {original_question}\"\n",
    "\n",
    "        assistant_response = f\"**Рассуждение:**\\n{cleaned_reasoning}\\n**Ответ: {model_answer}**\"\n",
    "\n",
    "        sample[\"system_prompt\"] = system_prompt\n",
    "        sample[\"user_prompt\"] = user_prompt\n",
    "        sample[\"messages\"] = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "        ]\n",
    "        sample[\"original_question\"] = original_question\n",
    "        # print(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ac15a2-a029-4c23-ab94-891e4c6f45f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T14:57:31.898923Z",
     "iopub.status.busy": "2025-07-24T14:57:31.898489Z",
     "iopub.status.idle": "2025-07-24T14:57:32.015120Z",
     "shell.execute_reply": "2025-07-24T14:57:32.014292Z",
     "shell.execute_reply.started": "2025-07-24T14:57:31.898900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cfd2a-4bf9-48ad-b2d5-2bf2f0fce94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, model_loader, data_processor, batch_size=8, output_path=\"evaluation_results.jsonl\"):\n",
    "        self.model_loader = model_loader\n",
    "        self.data_processor = data_processor\n",
    "        self.stats = {\"correct\": 0, \"incorrect\": 0, \"no_answer\": 0}\n",
    "        self.batch_size = batch_size\n",
    "        self.output_path = output_path\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_answer(generated_text):\n",
    "        match = re.search(r\"\\*\\*Ответ:\\s*([A-D])\", generated_text, re.IGNORECASE)\n",
    "        return match.group(1).upper() if match else None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_reasoning(generated_text):\n",
    "        match = re.search(r\"\\*\\*Ответ:\", generated_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return generated_text[:match.start()].strip()\n",
    "        return generated_text.strip()\n",
    "\n",
    "    def run(self, verbose=False):\n",
    "        model = self.model_loader.model\n",
    "        tokenizer = self.model_loader.tokenizer\n",
    "        dataset = self.data_processor.load_and_prepare_data()\n",
    "\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as log_file:\n",
    "            print(f\"Результаты будут сохранены в: {self.output_path}\")\n",
    "            progress_bar = tqdm(dataset.iter(batch_size=self.batch_size), desc=\"Оценка модели\", total=len(dataset) // self.batch_size)\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                messages_batch = batch[\"messages\"]\n",
    "\n",
    "                chat_templates = [\n",
    "                    tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "                    for msg in messages_batch\n",
    "                ]\n",
    "\n",
    "                encoded = tokenizer(\n",
    "                    chat_templates,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "\n",
    "                input_ids = encoded[\"input_ids\"].to(\"cuda\")\n",
    "                attention_mask = encoded[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=2048,\n",
    "                    use_cache=False,\n",
    "                    do_sample=False,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    repetition_penalty=1.1\n",
    "                )\n",
    "\n",
    "                generated_texts = tokenizer.batch_decode(outputs[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "                for i in range(len(generated_texts)):\n",
    "                    ground_truth = batch['model_answer'][i]\n",
    "                    full_generated_text = generated_texts[i]\n",
    "                    model_answer = self.extract_answer(full_generated_text)\n",
    "                    model_reasoning = self.extract_reasoning(full_generated_text)\n",
    "\n",
    "                    if model_answer is None:\n",
    "                        self.stats[\"no_answer\"] += 1\n",
    "                        result = \"NO_ANSWER\"\n",
    "                    elif model_answer == ground_truth:\n",
    "                        self.stats[\"correct\"] += 1\n",
    "                        result = \"CORRECT\"\n",
    "                    else:\n",
    "                        self.stats[\"incorrect\"] += 1\n",
    "                        result = \"INCORRECT\"\n",
    "\n",
    "                    log_entry = {\n",
    "                        \"id\": batch.get('id', ['N/A']*len(generated_texts))[i],\n",
    "                        \"original_question\": batch['original_question'][i],\n",
    "                        \"ground_truth\": ground_truth,\n",
    "                        \"model_reasoning\": model_reasoning,\n",
    "                        \"model_answer\": model_answer,\n",
    "                        \"result\": result,\n",
    "                        \"full_generated_text\": full_generated_text\n",
    "                    }\n",
    "                    log_file.write(json.dumps(log_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    '✅': self.stats['correct'],\n",
    "                    '❌': self.stats['incorrect'],\n",
    "                    '❓': self.stats['no_answer']\n",
    "                })\n",
    "\n",
    "        self.print_summary()\n",
    "\n",
    "    def print_summary(self):\n",
    "        total = sum(self.stats.values())\n",
    "        if total == 0:\n",
    "            print(\"Не было обработано ни одного сэмпла.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"#\" * 20 + \" Итоги оценки \" + \"#\" * 20)\n",
    "        print(f\"Всего обработано: {total}\")\n",
    "        print(f\"✅ Верных: {self.stats['correct']} ({self.stats['correct']/total:.2%})\")\n",
    "        print(f\"❌ Неверных: {self.stats['incorrect']} ({self.stats['incorrect']/total:.2%})\")\n",
    "        print(f\"❓ Без ответа: {self.stats['no_answer']} ({self.stats['no_answer']/total:.2%})\")\n",
    "        print(f\"Результаты в: {self.output_path}\")\n",
    "        print(\"#\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_MODEL_NAME = \"unsloth/Qwen3-4B-bnb-4bit\"\n",
    "    ADAPTER_PATH = \"/home/jupyter/datasphere/project/Anton_Qwen3-4B-bnb-4bit \"\n",
    "    DATASET_PATH = \"/home/jupyter/datasphere/project/alldata.jsonl\"#from 235b\n",
    "    OUTPUT_LOG_PATH = \"evaluation_results_qwen3_4b (call category)(new).jsonl\"\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    try:\n",
    "        model_loader = InferenceModelLoader(BASE_MODEL_NAME, ADAPTER_PATH)\n",
    "        data_processor = DatasetProcessor(DATASET_PATH)\n",
    "        evaluator = ModelEvaluator(model_loader, data_processor, batch_size=BATCH_SIZE, output_path=OUTPUT_LOG_PATH)\n",
    "        evaluator.run(verbose=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Ошибка: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"Проверьте пути, LoRA и BATCH_SIZE. При ошибке CUDA OOM — уменьшите батч.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
